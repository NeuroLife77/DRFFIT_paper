{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62678e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import mkdir, makedirs\n",
    "from datetime import datetime\n",
    "from copy import deepcopy as dcp\n",
    "import numpy as np\n",
    "import torch\n",
    "import joblib\n",
    "import pickle as pkl\n",
    "from lib.utils import *\n",
    "from lib.Wilson_Cowan.simulators import WC_stochastic_heun_PSD\n",
    "from lib.Wilson_Cowan.utils import *\n",
    "from lib.Wilson_Cowan.parameters_info import parameters_alpha_peak, parameters_range_bounds, parameters_lower_bound,parameters_upper_bound, parameter_names, parameters_original\n",
    "from lib.Wilson_Cowan.simulators import WC_stochastic_heun_PSD\n",
    "from lib.drffit.uniform_sampler import uniform_around_sampler as uniform_sampler\n",
    "from copy import deepcopy as dcp\n",
    "make_cluster_reals = True\n",
    "print('Parameters:\\n')\n",
    "for i, pn in enumerate(parameter_names):\n",
    "    print('\\t',i,'\\t',pn,':    \\t',parameters_lower_bound[i],'\\t',parameters_upper_bound[i])\n",
    "upper_bound = dcp(parameters_upper_bound)\n",
    "lower_bound = dcp(parameters_lower_bound)\n",
    "theta_min = lower_bound\n",
    "theta_range = upper_bound - lower_bound\n",
    "range_bounds = upper_bound - lower_bound\n",
    "# precompile simulator\n",
    "_, _, _ = WC_stochastic_heun_PSD(np.array([parameters_alpha_peak]),length = 6, dt=1,noise_seed=12, get_psd_I = False, remove_bad=True)\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.cluster import KMeans\n",
    "real = get_real_individual_PSD_scaled()\n",
    "set_mpl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133b8b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_data_path = '../Data/WC/initialization/'\n",
    "makedirs(glob_data_path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41c02c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_njobs = 10\n",
    "message = 'Generating a dataset as init and to train DRFFIT full space'\n",
    "\n",
    "# General simulation settings\n",
    "dt = 1.\n",
    "length = 302\n",
    "cutoff = [0,200] # 100Hz (resolution of 0.5Hz)\n",
    "noise_seed = np.random.randint(0,2**16)\n",
    "print(noise_seed)\n",
    "chunk_size, num_chunks = 10, 1000\n",
    "print(num_chunks)\n",
    "initial_conditions = [np.random.rand(2, chunk_size) for _ in range(num_chunks)]\n",
    "num_sim = chunk_size * num_chunks\n",
    "file_name = f'cube_{num_sim}_{count}'\n",
    "# Sampler settings\n",
    "search_width = 1.0\n",
    "\n",
    "point = theta_min + (theta_range/2)\n",
    "sampler = uniform_sampler(theta_min, theta_range = theta_range,sample_distribution='cube')\n",
    "sampler.set_state(point = point, width = search_width)\n",
    "parameters_samples = []\n",
    "\n",
    "for i in range(1):\n",
    "    for j in range(num_chunks//1):\n",
    "        parameters_samples.append(sampler.sample((chunk_size,))) \n",
    "\n",
    "all_samples = torch.cat(parameters_samples,dim = 0)\n",
    "print(len(parameters_samples))\n",
    "\n",
    "n_jobs = num_chunks\n",
    "if num_chunks > max_njobs:\n",
    "    n_jobs = max_njobs\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd182659",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24,10))\n",
    "for i in range(24):\n",
    "    ax = plt.subplot(3,8,i+1)\n",
    "    plt.violinplot(ensure_numpy(all_samples)[:,i])\n",
    "    plt.ylim([lower_bound[i],upper_bound[i]])\n",
    "    plt.title(f\"{parameter_names[i]}\")\n",
    "plt.suptitle('Distribution of parameter values over the samples')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975e0a6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize log of the search\n",
    "search_log_info = {}\n",
    "search_log_info['dt'] = dt\n",
    "search_log_info['length'] = length\n",
    "search_log_info['chunk_size'] = chunk_size\n",
    "search_log_info['num_chunks'] = num_chunks\n",
    "search_log_info['cutoff'] = cutoff\n",
    "search_log_info['noise_seed'] = noise_seed\n",
    "search_log_info['initial_conditions'] = initial_conditions\n",
    "search_log_info['message'] = message\n",
    "search_log_info['total_simulations'] = num_sim\n",
    "search_log_info['search_width'] = search_width\n",
    "search_log_info['min_norm'] = [-1]\n",
    "search_log_info['point'] = point\n",
    "\n",
    "\n",
    "# To keep track of round runtime\n",
    "runtime = datetime.now()-datetime.now()\n",
    "st_time = datetime.now()\n",
    "st_time_string = st_time.strftime('%D, %H:%M:%S')\n",
    "print(f'Start time: {st_time_string}')\n",
    "\n",
    "results = joblib.Parallel(n_jobs=n_jobs, verbose = 1)(joblib.delayed(WC_stochastic_heun_PSD)(\n",
    "    \n",
    "                                                                        parameters_samples[i],\n",
    "                                                                        length = length,\n",
    "                                                                        dt = dt,\n",
    "                                                                        initial_conditions = initial_conditions[i],\n",
    "                                                                        noise_seed = noise_seed,\n",
    "                                                                        PSD_cutoff = cutoff,\n",
    "                                                                        remove_bad = True\n",
    "    \n",
    "                                                        ) for i in range(num_chunks)\n",
    "                                        )\n",
    "# Group the simulations\n",
    "psd_E, _, pars = zip(*results)\n",
    "valid_PSD = []\n",
    "valid_pars = []\n",
    "for i in range(len(psd_E)):\n",
    "    if len(psd_E[i])>1:\n",
    "        valid_PSD.append(psd_E[i])\n",
    "        valid_pars.append(pars[i])\n",
    "\n",
    "# If no valid simulation remains\n",
    "if len(valid_PSD) == 0:\n",
    "    print(\"No valid simulations remained\")\n",
    "    search_log_info['valid_simulations'] = 0\n",
    "    search_log_info['message'] += ' (No valid simulations remained)'\n",
    "else:\n",
    "    simulated_samples = torch.cat(valid_PSD, dim = 0)\n",
    "    parameters_samples_good = torch.cat(valid_pars, dim = 0)\n",
    "    cleaned_PSD, cleaned_parameters = simulated_samples, parameters_samples_good\n",
    "    search_log_info['valid_simulations'] = cleaned_PSD.shape[0]\n",
    "\n",
    "# Define the log_info dict\n",
    "runtime = datetime.now()-st_time\n",
    "search_log_info['data'] = {'x':cleaned_PSD,'theta':cleaned_parameters}\n",
    "search_log_info['date'] = datetime.now().strftime('%D, %H:%M:%S')\n",
    "search_log_info['runtime'] = runtime\n",
    "\n",
    "# Keep track of the progress\n",
    "f_time_string = datetime.now().strftime('%D, %H:%M:%S')\n",
    "runtime_string = str(runtime)\n",
    "print(f'Finish time: {f_time_string}, Runtime: {runtime_string}')\n",
    "print(f\"Simulations shape: {cleaned_PSD.shape[0]}, {cleaned_PSD.shape[1]}\", end = '\\t')\n",
    "print(f\"Parameters shape: {cleaned_parameters.shape[0]}, {cleaned_parameters.shape[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f3416",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_log(search_log_info, glob_data_path, file_name, enforce_replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "real = get_real_individual_PSD_scaled(cutoff = 4, Hz = 80)\n",
    "target_reals = real\n",
    "target_PSDs_all_frequency_range = ensure_torch(target_reals).view(-1,156)\n",
    "target_PSDs_all_frequency_range /= target_PSDs_all_frequency_range.amax(1).view(-1,1)\n",
    "target_theta = ensure_torch(torch.zeros(target_PSDs_all_frequency_range.shape[0],24))\n",
    "print(f'Targets full range: \\tx: {target_PSDs_all_frequency_range.shape}\\t theta: {target_theta.shape}')\n",
    "print(target_PSDs_all_frequency_range.amax(1))\n",
    "train_x_all_freq = search_log_info['data']['x']\n",
    "train_theta = search_log_info['data']['theta']\n",
    "frequency_range = [4,160]\n",
    "target_PSDs = ensure_torch(target_PSDs_all_frequency_range[:,frequency_range[0]-4:frequency_range[1]-4])\n",
    "freq = [0.5*i for i in range(frequency_range[0],frequency_range[1])]\n",
    "train_x = train_x_all_freq[:,frequency_range[0]:frequency_range[1]].float()\n",
    "train_x /= torch.amax(train_x,1).view(-1,1)\n",
    "print(f'To fit shape: \\tx: {train_x.shape}\\t theta: {train_theta.shape}')\n",
    "print(train_x.amax(1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca42718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "all_errs = []\n",
    "initial_points = []\n",
    "loss_fn = 'correlation'\n",
    "sorted_target = []\n",
    "target_PSDs = target_PSDs.to('cuda')\n",
    "train_x = train_x.to('cuda')\n",
    "for i in tqdm(range(target_PSDs.shape[0])):\n",
    "    error = correlation_loss_fn(target_PSDs[i].view(1,-1),train_x)\n",
    "    err_index = torch.argmin(error).cpu()\n",
    "    sorted_target.append({\n",
    "            'theta':train_theta[err_index].view(1,-1),\n",
    "            'x': train_x[err_index].cpu().view(1,-1),\n",
    "            'error':error[err_index].cpu().view(1,-1),\n",
    "            'target':target_PSDs[i].cpu().view(1,-1),\n",
    "            'target_theta':target_theta[i].view(1,-1),\n",
    "            'real_index':torch.tensor(i).view(1,-1)\n",
    "    })\n",
    "    all_errs.append(error[err_index].data.item())\n",
    "all_errs = torch.tensor(all_errs)\n",
    "sorted_errors = torch.argsort(all_errs, descending = True)\n",
    "sorted_target_info = []\n",
    "for i in sorted_errors:\n",
    "    sorted_target_info.append(sorted_target[i])\n",
    "\n",
    "target_log = {\n",
    "    'original': {'x': target_PSDs.cpu(),'theta':target_theta},\n",
    "    'sorted':{\n",
    "        'data':{'source':glob_data_path, 'x': train_x.cpu(), 'theta':train_theta},\n",
    "        'loss_fn': loss_fn,\n",
    "        'indices':sorted_errors,\n",
    "    }\n",
    "}\n",
    "\n",
    "target_log['original']['fits'] = {}\n",
    "target_log['original']['fits']['theta'] = torch.cat([sorted_target[i]['theta'] for i in range(target_PSDs.shape[0])], dim = 0)\n",
    "target_log['original']['fits']['x'] = torch.cat([sorted_target[i]['x'] for i in range(target_PSDs.shape[0])], dim = 0)\n",
    "target_log['original']['fits']['error'] = torch.cat([sorted_target[i]['error'] for i in range(target_PSDs.shape[0])], dim = 0)\n",
    "target_log['original']['fits']['target'] = torch.cat([sorted_target[i]['target'] for i in range(target_PSDs.shape[0])], dim = 0)\n",
    "target_log['original']['fits']['target_theta'] = torch.cat([sorted_target[i]['target_theta'] for i in range(target_PSDs.shape[0])], dim = 0)\n",
    "\n",
    "\n",
    "target_log['sorted']['worst_info_all'] = {}\n",
    "target_log['sorted']['worst_info_all']['theta'] = torch.cat([sorted_target_info[i]['theta'] for i in range(target_PSDs.shape[0])], dim = 0)\n",
    "target_log['sorted']['worst_info_all']['x'] = torch.cat([sorted_target_info[i]['x'] for i in range(target_PSDs.shape[0])], dim = 0)\n",
    "target_log['sorted']['worst_info_all']['error'] = torch.cat([sorted_target_info[i]['error'] for i in range(target_PSDs.shape[0])], dim = 0)\n",
    "target_log['sorted']['worst_info_all']['target'] = torch.cat([sorted_target_info[i]['target'] for i in range(target_PSDs.shape[0])], dim = 0)\n",
    "target_log['sorted']['worst_info_all']['target_theta'] = torch.cat([sorted_target_info[i]['target_theta'] for i in range(target_PSDs.shape[0])], dim = 0)\n",
    "target_log['sorted']['worst_info_all']['real_index'] = torch.cat([sorted_target_info[i]['real_index'] for i in range(target_PSDs.shape[0])], dim = 0)\n",
    "target_log['original']['reorder'] = torch.argsort(target_log['sorted']['worst_info_all']['real_index'][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5744f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_log['sorted']['worst_info_all']['x'].shape)   \n",
    "print(target_log['sorted']['data']['x'].shape)   \n",
    "print()\n",
    "print(target_log['sorted']['worst_info_all']['error'].mean())\n",
    "print(target_log['sorted']['worst_info_all']['error'][:40].mean())\n",
    "print(target_log['sorted']['worst_info_all']['error'][:40].min())\n",
    "print()\n",
    "print(target_log['sorted']['worst_info_all']['error'].max())\n",
    "print(target_log['sorted']['worst_info_all']['error'].min())\n",
    "print()\n",
    "print((target_log['sorted']['worst_info_all']['target'][target_log['original']['reorder']] == target_log['original']['x']).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8935e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_log(target_log, glob_data_path, f'{file_name}_fits')\n",
    "count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b3cb373ab163b43349ea32823e4187dc37f8a5f4a04aa5ed18ab49bf23b1de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
